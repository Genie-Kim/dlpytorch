{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why word vectors?\n",
    "\n",
    "When working with words, dealing with the huge but sparse domain of language can be challenging. Even for a small corpus, your neural network (or any type of model) needs to support many thousands of discrete inputs and outputs.\n",
    "\n",
    "Besides the raw number words, the standard technique of representing words as one-hot vectors (e.g. \"the\" = `[0 0 0 1 0 0 0 0 ...]`) does not capture any information about relationships between words.\n",
    "\n",
    "There are a few techniques for creating word vectors. The word2vec algorithm predicts words in a context (e.g. what is the most likely word to appear in \"the cat ? the mouse\"), while GloVe vectors are based on global counts across the corpus.\n",
    "\n",
    "# GloVe: Global Vectors for Word Representation\n",
    "Created at the Stanford NLP group by: Jeffrey Pennington,   Richard Socher,   Christopher D. Manning\n",
    "\n",
    "Word vectors address this problem by representing words in a multi-dimensional vector space. This can bring the dimensionality of the problem from hundreds-of-thousands to just hundreds. Plus, the vector space is able to capture semantic relationships between words in terms of distance and vector arithmetic.\n",
    "\n",
    "![](https://i.imgur.com/y4hG1ak.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Torchtext includes functions to download GloVe (and other) embeddings'''\n",
    "import torch\n",
    "import torchtext.vocab as vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:09, 6.64MB/s]                               \n",
      "100%|██████████| 400000/400000 [00:14<00:00, 27400.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 words\n"
     ]
    }
   ],
   "source": [
    "glove = vocab.GloVe(name='6B', dim=100)\n",
    "\n",
    "print('Loaded {} words'.format(len(glove.itos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned `GloVe` object includes attributes:\n",
    "- `stoi` _string-to-index_ returns a dictionary of words to indexes\n",
    "- `itos` _index-to-string_ returns an array of words by index\n",
    "- `vectors` returns the actual vectors. To get a word vector get the index to get the vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding closest vectors\n",
    "\n",
    "Going from word to vector is easy enough, but to go from vector to word takes more work. Here I'm calculating the distance for each word in the vocabulary, and sorting based on that distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(word):\n",
    "    return glove.vectors[glove.stoi[word]]\n",
    "\n",
    "def closest_readable_implementation(vec, n=10):\n",
    "    all_dists=[]\n",
    "    for w in glove.itos:\n",
    "        distance = torch.dist(vec, get_word(w))\n",
    "        all_dists.append([w, distance])\n",
    "    return sorted(all_dists, key=lambda t: t[1])[:n]\n",
    "\n",
    "\n",
    "def closest(vec, n=10):\n",
    "    all_dists = [(w, torch.dist(vec, get_word(w))) for w in glove.itos]\n",
    "    return sorted(all_dists, key=lambda t: t[1])[:n]\n",
    "\n",
    "# A helper function to print that list\n",
    "def print_tuples(tuples):\n",
    "    for tuple in tuples:\n",
    "        print('(%.4f) %s' % (tuple[1], tuple[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0000) c++\n",
      "(4.3938) compiler\n",
      "(4.6451) fortran\n",
      "(4.6805) compilers\n",
      "(4.9288) objective-c\n",
      "(4.9339) javascript\n",
      "(5.0873) php\n",
      "(5.1683) object-oriented\n",
      "(5.2860) perl\n",
      "(5.3822) cobol\n"
     ]
    }
   ],
   "source": [
    "'''Now using a known word vector we can see which other vectors are closest:'''\n",
    "print_tuples(closest(get_word('c++')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word analogies with vector arithmetic\n",
    "\n",
    "The most interesting feature of a well-trained word vector space is that certain semantic relationships can be captured with regular vector arithmetic. \n",
    "\n",
    "![](https://i.imgur.com/d0KuM5x.png)\n",
    "\n",
    "(image borrowed from [a slide from Omer Levy and Yoav Goldberg](https://levyomer.wordpress.com/2014/04/25/linguistic-regularities-in-sparse-and-explicit-word-representations/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the form w1 : w2 :: w3 : ?\n",
    "def analogy(w1, w2, w3):\n",
    "    print('\\n[%s : %s :: %s : ?]' % (w1, w2, w3))\n",
    "    # w2 - w1 + w3 = w4\n",
    "    closest_words = closest(get_word(w2) - get_word(w1) + get_word(w3))\n",
    "    # filter out input words\n",
    "    closest_words = [t for t in closest_words if t[0] not in [w1, w2, w3]]\n",
    "    print_tuples(closest_words[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[king : man :: queen : ?]\n",
      "(4.0811) woman\n",
      "(4.6916) girl\n",
      "(5.2703) she\n",
      "(5.2788) teenager\n"
     ]
    }
   ],
   "source": [
    "'''The classic example:'''\n",
    "analogy('king', 'man', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[man : actor :: woman : ?]\n",
      "(2.8133) actress\n",
      "(5.0039) comedian\n",
      "(5.1399) actresses\n",
      "(5.2773) starred\n",
      "\n",
      "[cat : kitten :: dog : ?]\n",
      "(3.8146) puppy\n",
      "(4.2944) rottweiler\n",
      "(4.5888) puppies\n",
      "(4.6086) pooch\n",
      "\n",
      "[russia : moscow :: canada : ?]\n",
      "(4.3024) montreal\n",
      "(4.3245) toronto\n",
      "(4.4689) winnipeg\n",
      "(4.5301) ontario\n",
      "\n",
      "[rich : mansion :: poor : ?]\n",
      "(5.8262) residence\n",
      "(5.9444) riverside\n",
      "(6.0283) hillside\n",
      "(6.0328) abandoned\n",
      "\n",
      "[paper : newspaper :: screen : ?]\n",
      "(4.7810) tv\n",
      "(5.1049) television\n",
      "(5.3818) cinema\n",
      "(5.5524) feature\n",
      "\n",
      "[earth : moon :: sun : ?]\n",
      "(6.2294) lee\n",
      "(6.4125) kang\n",
      "(6.4644) tan\n",
      "(6.4757) yang\n",
      "\n",
      "[house : roof :: castle : ?]\n",
      "(6.2919) stonework\n",
      "(6.3779) masonry\n",
      "(6.4773) canopy\n",
      "(6.4954) fortress\n",
      "\n",
      "[building : architect :: software : ?]\n",
      "(5.8369) programmer\n",
      "(6.8881) entrepreneur\n",
      "(6.9240) inventor\n",
      "(6.9730) developer\n",
      "\n",
      "[virginia : richmond :: california : ?]\n",
      "(4.3444) pasadena\n",
      "(4.3696) francisco\n",
      "(4.3829) angeles\n",
      "(4.5840) mesa\n",
      "\n",
      "[good : heaven :: bad : ?]\n",
      "(4.3959) hell\n",
      "(5.2864) ghosts\n",
      "(5.2898) hades\n",
      "(5.3414) madness\n",
      "\n",
      "[jordan : basketball :: messi : ?]\n",
      "(6.8756) maradona\n",
      "(6.8879) soccer\n",
      "(6.9534) romario\n",
      "(7.0380) juniors\n"
     ]
    }
   ],
   "source": [
    "'''Now let's explore the word space and see what stereotypes we can uncover'''\n",
    "analogy('man', 'actor', 'woman')\n",
    "analogy('cat', 'kitten', 'dog')\n",
    "analogy('russia', 'moscow', 'canada')\n",
    "analogy('rich', 'mansion', 'poor')\n",
    "analogy('paper', 'newspaper', 'screen')\n",
    "analogy('earth', 'moon', 'sun') # Interesting failure mode\n",
    "analogy('house', 'roof', 'castle')\n",
    "analogy('building', 'architect', 'software')\n",
    "analogy('virginia', 'richmond', 'california')\n",
    "analogy('good', 'heaven', 'bad')\n",
    "analogy('jordan', 'basketball', 'messi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ibm : software :: citigroup : ?]\n",
      "(5.1651) banking\n",
      "(5.2357) credit\n",
      "(5.4298) merrill\n",
      "(5.4445) banks\n"
     ]
    }
   ],
   "source": [
    "analogy('ibm', 'software', 'citigroup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ibm : computer :: chrysler : ?]\n",
      "(4.4520) auto\n",
      "(5.0944) car\n",
      "(5.2303) vehicle\n",
      "(5.3724) utility\n"
     ]
    }
   ],
   "source": [
    "analogy('ibm', 'computer', 'chrysler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
