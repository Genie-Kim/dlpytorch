{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation with a Sequence to Sequence Network and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() # There is no GPU on this instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing words\n",
    "\n",
    "We'll need a unique index per word to use as the inputs and targets of the networks later. To keep track of all this we will use a helper class called `Lang` which has word &rarr; index (`word2index`) and index &rarr; word (`index2word`) dictionaries, as well as a count of each word `word2count` to use to later replace rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2 # Count SOS and EOS\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and decoding files\n",
    "\n",
    "The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split lines into pairs. The files are all English &rarr; Other Language, so if we want to translate from Other Language &rarr; English I added the `reverse` flag to reverse the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('/var/local/%s-%s.txt' % (lang1, lang2)).read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering sentences\n",
    "\n",
    "Since there are a *lot* of example sentences and we want to train something quickly, we'll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes punctuation) and we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc. (accounting for apostrophes being removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "good_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \"\n",
    ")\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(good_prefixes)\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "* Read text file and split into lines, split lines into pairs\n",
    "* Normalize text, filter by length and content\n",
    "* Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 154883 sentence pairs\n",
      "Trimmed to 10006 sentence pairs\n",
      "Indexing words...\n",
      "['vous frissonnez . vous avez froid ?', 'you re shivering . are you cold ?']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('eng', 'fra', True)\n",
    "\n",
    "# Print an example pair\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning training data into Tensors/Variables\n",
    "\n",
    "To train we need to turn the sentences into something the neural network can understand, which of course means numbers. Each sentence will be split into words and turned into a Tensor, where each word is replaced with the index (from the Lang indexes made earlier). While creating these tensors we will also append the EOS token to signal that the sentence is over.\n",
    "\n",
    "![](https://i.imgur.com/LzocpGH.png)\n",
    "\n",
    "A Tensor is a multi-dimensional array of numbers, defined with some type e.g. FloatTensor or LongTensor. In this case we'll be using LongTensor to represent an array of integer indexes.\n",
    "\n",
    "Trainable PyTorch modules take Variables as input, rather than plain Tensors. A Variable is basically a Tensor that is able to keep track of the graph state, which is what makes autograd (automatic calculation of backwards gradients) possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "#     print('var =', var)\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "\n",
    "<img src=\"images/encoder-network.png\" style=\"float: right\" />\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Bahdanau et al. model\n",
    "\n",
    "In summary our decoder should consist of four main parts - an embedding layer turning an input word into a vector; a layer to calculate the attention energy per encoder output; a RNN layer; and an output layer.\n",
    "\n",
    "The decoder's inputs are the last RNN hidden state $s_{i-1}$, last output $y_{i-1}$, and all encoder outputs $h_*$.\n",
    "\n",
    "* embedding layer with inputs $y_{i-1}$\n",
    "    * `embedded = embedding(last_rnn_output)`\n",
    "* attention layer $a$ with inputs $(s_{i-1}, h_j)$ and outputs $e_{ij}$, normalized to create $a_{ij}$\n",
    "    * `attn_energies[j] = attn_layer(last_hidden, encoder_outputs[j])`\n",
    "    * `attn_weights = normalize(attn_energies)`\n",
    "* context vector $c_i$ as an attention-weighted average of encoder outputs\n",
    "    * `context = sum(attn_weights * encoder_outputs)`\n",
    "* RNN layer(s) $f$ with inputs $(s_{i-1}, y_{i-1}, c_i)$ and internal hidden state, outputting $s_i$\n",
    "    * `rnn_input = concat(embedded, context)`\n",
    "    * `rnn_output, rnn_hidden = rnn(rnn_input, last_hidden)`\n",
    "* an output layer $g$ with inputs $(y_{i-1}, s_i, c_i)$, outputting $y_i$\n",
    "    * `output = out(embedded, rnn_output, context)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = GeneralAttn(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note that we will only be running forward for a single decoder time step, but will use all encoder outputs\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Luong et al. model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size, max_length=MAX_LENGTH):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        return F.softmax(attn_energies).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            #print(energy.size(),hidden.size())\n",
    "            energy = hidden.view(-1).dot(energy.view(-1))\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.other.dot(energy)\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now we can build a decoder that plugs this Attn module in after the RNN to calculate attention weights,\n",
    "apply those weights to the encoder outputs to get a context vector'''\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        \n",
    "        # Combine embedded input word and last context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the models\n",
    "\n",
    "To make sure the Encoder and Decoder model are working (and working together) we'll do a quick test with fake word inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (gru): GRU(10, 10, num_layers=2)\n",
      ")\n",
      "AttnDecoderRNN(\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (gru): GRU(20, 10, num_layers=2, dropout=0.1)\n",
      "  (out): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (attn): Attn(\n",
      "    (attn): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 10]) torch.Size([2, 1, 10]) torch.Size([1, 1, 3])\n",
      "torch.Size([1, 10]) torch.Size([2, 1, 10]) torch.Size([1, 1, 3])\n",
      "torch.Size([1, 10]) torch.Size([2, 1, 10]) torch.Size([1, 1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "'''To make sure the Encoder and Decoder model are working (and working together)\n",
    "we'll do a quick test with fake word inputs:'''\n",
    "encoder_test = EncoderRNN(10, 10, 2)\n",
    "decoder_test = AttnDecoderRNN('general', 10, 10, 2)\n",
    "print(encoder_test)\n",
    "print(decoder_test)\n",
    "\n",
    "encoder_hidden = encoder_test.init_hidden()\n",
    "word_input = Variable(torch.LongTensor([1, 2, 3]))\n",
    "if USE_CUDA:\n",
    "    encoder_test.cuda()\n",
    "    word_input = word_input.cuda()\n",
    "encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "\n",
    "word_inputs = Variable(torch.LongTensor([1, 2, 3]))\n",
    "decoder_attns = torch.zeros(1, 3, 3)\n",
    "decoder_hidden = encoder_hidden\n",
    "decoder_context = Variable(torch.zeros(1, decoder_test.hidden_size))\n",
    "\n",
    "if USE_CUDA:\n",
    "    decoder_test.cuda()\n",
    "    word_inputs = word_inputs.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "for i in range(3):\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attn = decoder_test(word_inputs[i], decoder_context, decoder_hidden, encoder_outputs)\n",
    "    print(decoder_output.size(), decoder_hidden.size(), decoder_attn.size())\n",
    "    decoder_attns[0, i] = decoder_attn.squeeze(0).cpu().data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Defining a training iteration\n",
    "\n",
    "To train we first run the input sentence through the encoder word by word, and keep track of every output and the latest hidden state. Next the decoder is given the last hidden state of the decoder as its first hidden state, and the `<SOS>` token as its first input. From there we iterate to predict a next token from the decoder.\n",
    "\n",
    "### Teacher Forcing and Scheduled Sampling\n",
    "\n",
    "\"Teacher Forcing\", or maximum likelihood sampling, means using the real target outputs as each next input when training. The alternative is using the decoder's own guess as the next input. Using teacher forcing may cause the network to converge faster, but [when the trained network is exploited, it may exhibit instability](http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf).\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with coherent grammar but wander far from the correct translation - you could think of it as having learned how to listen to the teacher's instructions, without learning how to venture out on its own.\n",
    "\n",
    "The solution to the teacher-forcing \"problem\" is known as [Scheduled Sampling](https://arxiv.org/abs/1506.03099), which simply alternates between using the target values and predicted values when training. We will randomly choose to use teacher forcing with an if statement while training - sometimes we'll feed use real target as the input (ignoring the decoder's output), sometimes we'll use the decoder's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    # Choose whether to use teacher forcing\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "        \n",
    "        # Teacher forcing: Use the ground-truth target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output[0].view(1,-1), target_variable[di])\n",
    "            decoder_input = target_variable[di] # Next target is next input\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use network's own prediction as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            #print(decoder_output[0].size(), target_variable[di].size())\n",
    "            loss += criterion(decoder_output[0].view(1,-1), target_variable[di])\n",
    "            \n",
    "            # Get most likely word index (highest value) from output\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            \n",
    "            decoder_input = Variable(torch.LongTensor([[ni]])) # Chosen word is next input\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "            # Stop at end of sentence (not necessary when using known targets)\n",
    "            if ni == EOS_token: break\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Finally helper functions to print time elapsed and estimated time remaining, given the current time and progress.'''\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running training\n",
    "\n",
    "Initialize a network and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models, optimizers, and a loss function (criterion).\n",
    "attn_model = 'general'\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout_p = 0.05\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers)\n",
    "decoder = AttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "# Then set up variables for plotting and tracking progress:\n",
    "n_epochs = 50000\n",
    "plot_every = 200\n",
    "print_every = 1000\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Begin training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    training_pair = variables_from_pair(random.choice(pairs))\n",
    "    input_variable = training_pair[0]\n",
    "    target_variable = training_pair[1]\n",
    "\n",
    "    # Run the train function\n",
    "    loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0: continue\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training loss\n",
    "\n",
    "Plotting is done with matplotlib, using the array `plot_losses` that was created while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae93740828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUmoE+n/GQ0vYEKDNQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae9116d160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the network\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets. Instead we always feed the decoder's predictions back to itself. Every time it predicts a word, we add it to the output string. If it predicts the EOS token we stop there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:'''\n",
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    output_words, decoder_attn = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je suis ambitieux .\n",
      "= i m ambitious .\n",
      "< i m ambitious . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
